#*
 Copyright 2013-2014 Norconex Inc.
 
 This file is part of Norconex Filesystem Collector.
 
 Norconex Filesystem Collector is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.
 
 Norconex Filesystem Collector is distributed in the hope that it will be useful, 
 but WITHOUT ANY WARRANTY; without even the implied warranty of 
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.
 
 You should have received a copy of the GNU General Public License
 along with Norconex Filesystem Collector. If not, see <http://www.gnu.org/licenses/>.
*#
#set($h1 = '#')
#set($h2 = '##')
#set($h3 = '###')
#set($h4 = '####')
#set($docpath = "./apidocs/com/norconex/importer")
<head><title>Configuration</title><meta name="Author" content="Norconex Inc." /></head>


$h2 Configuration Quick start

To get started quickly, download the latest version of 
Norconex Filesystem Collector and locate the file 
[./examples/HOWTO_RUN_EXAMPLES.txt](./examples/HOWTO_RUN_EXAMPLES.txt).  
This file will point you to a functional configuration file and will have you 
running a very simple crawler in no time.  These sample files are also made 
available here for your convenience:

  * [Functional Configuration Sample](./examples/sample-config.xml)
  * [Self-Documented Configuration Reference](./examples/collector-filesystem-config-reference.xml)


$h2 Filesystem Collector Configuration Options

To get the full potential of Norconex Filesystem Collector and learn which parts 
can easily be extended, refer to the following for an XML-based configuration.
Entries with a "class" attribute are expecting an implementation
of your choice.   The Filesystem Collector API offers several concrete
implementations already.  Developers can also create their own
by implementing the proper Java interfaces. Refer to the
[Norconex Filesystem Collector JavaDoc](./apidocs/index.html)
and/or see further down what interfaces you can implement.
Go to the
[Extend the Filesystem Collector](./usage.html#Extend_the_Filesystem_Collector)
page for more details on adding your own implementations.

    <fscollector id="...">
    
      <progressDir>...</progressDir>
      <logsDir>...</logsDir>
    
      <crawlerDefaults>
        <startPaths
          <path>...</path>
          <pathsFile>...</pathsFile>
          ...
        </startPaths>
        <workDir>...</workDir>
        <numThreads>...</numThreads>
        <maxDocuments>...</maxDocuments>
        <keepDownloads>...</keepDownloads>
        <orphansStrategy>...</orphansStrategy>
        <crawlerListeners>
          <listener class="..."/>
          ...
        </crawlerListeners>
        <crawlDataStoreFactory class="" />
        <referenceFilters>
          <filter class="..." />
          ...
        </referenceFilters>
        <metadataFilters>
          <filter class="..." />
          ...
        </metadataFilters>
        <metadataChecksummer class="..." />
        <documentFilters>
          <filter class="..." />
          ...
        </documentFilters>
        <preImportProcessors>
          <processor class="..." />
          ...
        </preImportProcessors>
    
        <importer>
           <!-- refer to Importer documentation -->
        </importer>
    
        <documentChecksummer class="..." />
 
         <postImportProcessors>
          <processor class="..."></processor>
        </postImportProcessors>
    
        <committer class="..." />
      </crawlerDefaults>
    
      <crawlers>
        <crawler id="...">
           <!-- All default crawler options are the same you use can use 
                here (overwriting defaults).  You need to define at least one
                crawler. -->
        </crawler>
        ...
      </crawlers>
    
    </fscollector>

The table below lists interface names that you can easily extend, and
also lists available out-of-the-box implementations.

In the configuration file, **you have to use the fully qualified name**,
as defined in the Javadoc (you can use variables to shorten package names).  
Click on a class or interface name to go directly
to its full documentation, with extra configuration options.

When a default implementation exists for a configuration option taking
a "class" attribute, the default implementations is <u>underlined</u>.
    
#set($fileAPI="./apidocs/com/norconex/collector/fs")
#set($coreAPI="../collector-core/apidocs/com/norconex/collector/fs")

<table>
 <thead>
  <tr>
   <th>Tag</th>
   <th>Description</th>
   <th>Classes</th>
   <th>Interface</th>
  </tr>
 </thead>
 <tbody>
   <tr>
     <td>fscollector</td>
     <td>Root tag, you must give your configuration a unique identifier value.</td>
     <td>N/A</td>
     <td>N/A</td>
   </tr>
   <tr>
     <td>progressDir</td>
     <td>Directory where to store crawling progress files.  Default is "./progress".</td>
     <td>N/A</td>
     <td>N/A</td>
   </tr>
   <tr>
     <td>logsDir</td>
     <td>Directory where crawl logs will be stored.  Default is "./logs".</td>
     <td>N/A</td>
     <td>N/A</td>
   </tr>
   <tr>
     <td>startPaths</td>
     <td>One or more &lt;path&gt; or files containing paths to start crawling from.
         Try to use in combination with filters.</td>
     <td>N/A</td>
     <td>N/A</td>
   </tr>
    <tr>
     <td>numThreads</td>
     <td>Number of execution threads for a crawler.  Default is 2.</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>maxDocuments</td>
     <td>Maximum files to successfully process.  Default is -1 (unlimited).</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>workDir</td>
     <td>Where to store files created as part of crawling activies.  Default is "./work".</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>keepDownloads</td>
     <td>Whether to keep downloaded files. Defaut is false.</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>orphansStrategy</td>
     <td>What to do with urls not being referenced anymore. IGNORE (default), DELETE, or PROCESS.</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>crawlerListeners</td>
     <td>Listen to crawling events.</td>
     <td></td>
     <td><a href="$coreAPI/crawler/event/ICrawlerEventListener.html">ICrawlerEventListener</a></td>
    </tr>
    <tr>
     <td>crawlDataStoreFactory</td>
     <td>URLs and crawl-related information data store.</td>
     <td>
       <u><a href="$coreAPI/data/store/impl/mapdb/MapDBCrawlDataStoreFactory.html">MapDBCrawlDataStoreFactory</a></u>,
       <a href="$fileAPI/data/store/impl/jdbc/JDBCCrawlDataStoreFactory.html">JDBCCrawlDataStoreFactory</a>,
       <a href="$fileAPI/data/store/impl/mongo/MongoCrawlDataStoreFactory.html">MongoCrawlDataStoreFactory</a>
     </td>
     <td><a href="$coreAPI/data/store/ICrawlDataStoreFactory.html">ICrawlDataStoreFactory</a></td>
    </tr>
    <tr>
     <td>referenceFilters</td>
     <td>Filter based on refereences (i.e. URLs).</td>
     <td>
       <a href="$coreAPI/filter/impl/ExtensionReferenceFilter.html">ExtensionReferenceFilter</a>,
       <a href="$coreAPI/filter/impl/RegexReferenceFilter.html">RegexReferenceFilter</a>,
     </td>
     <td><a href="$coreAPI/filter/IReferenceFilter.html">IReferenceFilter</a></td>
    </tr>
    <tr>
     <td>metadataFilters</td>
     <td>Filter based on file properties.</td>
     <td>
       <a href="$coreAPI/filter/impl/ExtensionReferenceFilter.html">ExtensionReferenceFilter</a>,
       <a href="$coreAPI/filter/impl/RegexReferenceFilter.html">RegexReferenceFilter</a>,
       <a href="$coreAPI/filter/impl/RegexMetadataFilter.html">RegexMetadataFilter</a>,
     </td>
     <td><a href="$coreAPI/filter/IMetadataFilter.html">IMetadataFilter</a></td>
    </tr>
    <tr>
     <td>metadataChecksummer</td>
     <td>Create document checksum from file properties.</td>
     <td><u><a href="$fileAPI/checksum/impl/FileMetadataChecksummer.html">FileMetadataChecksummer</a></u></td>
     <td><a href="$coreAPI/checksum/IMetadataChecksummer.html">IMetadataChecksummer</a></td>
    </tr>
    <tr>
     <td>documentFilters</td>
     <td>Filter documents.</td>
     <td>
       <a href="$coreAPI/filter/impl/ExtensionReferenceFilter.html">ExtensionReferenceFilter</a>,
       <a href="$coreAPI/filter/impl/RegexReferenceFilter.html">RegexReferenceFilter</a>,
       <a href="$coreAPI/filter/impl/RegexMetadataFilter.html">RegexMetadataFilter</a>,
     </td>
     <td><a href="$coreAPI/filter/IDocumentFilter.html">IDocumentFilter</a></td>
    </tr>
    <tr>
     <td>preImportProcessors</td>
     <td>Process a document before import.</td>
     <td></td>
     <td><a href="$fileAPI/doc/IFileDocumentProcessor.html">IFileDocumentProcessor</a></td>
    </tr>
    <tr style="color: black;">
     <td><b>importer</b></td>
     <td>
       Performs document text extraction and manipulation.  It has many features
       and many 
       <a href="http://www.norconex.com/product/importer/formats.html">file formats</a> 
       are supported. Refer to 
       <a href="http://www.norconex.com/product/importer/configuration.html">Importer configuration options</a>.
     </td>
     <td></td>
     <td></td>
    </tr>
    <tr>
     <td>documentChecksummer</td>
     <td>Create a checksum from document.</td>
     <td><u><a href="$coreAPI/checksum/impl/MD5DocumentChecksummer.html">MD5DocumentChecksummer</a></u></td>
     <td><a href="$coreAPI/checksum/IDocumentChecksummer.html">IDocumentChecksummer</a></td>
    </tr>
    <tr>
     <td>postImportProcessors</td>
     <td>Process a document after import.</td>
     <td></td>
     <td><a href="$fileAPI/doc/IFileDocumentProcessor.html">IFileDocumentProcessor</a></td>
    </tr>
    <tr style="color: black;">
     <td><b>committer</b></td>
     <td>
       Where to commit a document when processed.  Different implementations are available. 
       Refer to <a href="http://www.norconex.com/product/committer">Committer module</a>.
     </td>
     <td></td>
     <td></td>
    </tr>
    <tr>
     <td>crawler</td>
     <td>Define as many crawlers as you like.  They must each have a unique identifier.</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
 </tbody>
</table>

$h2 Importer Configuration Options

The [Importer](http://www.norconex.com/product/importer/) module
is an integral part of the Filesystem Collector.  It is reponsible for extracting
text out of documents. It also provides document manipulation options and 
filtering options.  Much more is found in this module distributed with the Filesystem Collector.   
Read the [Importer Configuration Options](http://www.norconex.com/product/importer/configuration.html).

$h2 Committer Configuration Options

The [Committer](http://www.norconex.com/product/committer/) module is 
responsible for taking the text extracted out of 
your collected documents and submit it to your target repository (e.g. 
search engine).  Make sure you download a 
[Committer implementation](http://www.norconex.com/product/committer/#list)
matching your target repository.  Configuration options for is specific
to each committers.  Refer to your committer documentation.


$h2 More Options

There is a lot more you can do to structure your configuration files
the way you like.  Refer to this
[additional documentation](../commons-lang/apidocs/com/norconex/commons/lang/config/ConfigurationLoader.html)
for more configuration options such as creating reusable
configuration fragments and using variables to make your files easier
to maintain and more portable across different environments.

